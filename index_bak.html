<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Linchao Zhu</title>
<meta name="description" content="A minimal Jekyll theme for your blog by designer Michael Rose.">
<meta name="keywords" content="Jekyll, theme, responsive, blog, template">


<!-- Twitter Cards -->
<meta name="twitter:title" content="Linchao Zhu">
<meta name="twitter:description" content="A minimal Jekyll theme for your blog by designer Michael Rose.">



<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ffmpbgrnn.github.io//images/default-thumb.png">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Linchao Zhu">
<meta property="og:description" content="A minimal Jekyll theme for your blog by designer Michael Rose.">
<meta property="og:url" content="https://ffmpbgrnn.github.io//">
<meta property="og:site_name" content="Linchao Zhu">

<meta property="og:image" content="https://ffmpbgrnn.github.io//images/default-thumb.png">






<link rel="canonical" href="https://ffmpbgrnn.github.io//">
<link href="https://ffmpbgrnn.github.io//feed.xml" type="application/atom+xml" rel="alternate" title="Linchao Zhu Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://ffmpbgrnn.github.io//assets/css/main.css">

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="https://ffmpbgrnn.github.io//assets/js/vendor/html5shiv.min.js"></script>
	<script src="https://ffmpbgrnn.github.io//assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="https://ffmpbgrnn.github.io//assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://ffmpbgrnn.github.io//favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://ffmpbgrnn.github.io//favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://ffmpbgrnn.github.io//images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://ffmpbgrnn.github.io//images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://ffmpbgrnn.github.io//images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://ffmpbgrnn.github.io//images/apple-touch-icon-144x144-precomposed.png">

</head>

<body class="page">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="https://ffmpbgrnn.github.io//">Linchao Zhu</a>
	</div><!-- /.site-name -->
	<!--<div class="top-navigation">-->
		<!--<nav role="navigation" id="site-nav" class="nav">-->
			<!--<ul>-->
				<!---->
			<!--</ul>-->
		<!--</nav>-->
	<!--</div>[> /.top-navigation <]-->
</div><!-- /.navigation-wrapper -->




<div id="main" role="main">
  <div class="article-author-side">
    

<div itemscope itemtype="http://schema.org/Person">

  <!--<h3 itemprop="name">Linchao Zhu</h3>-->
  <!--<p></p>-->
  <a href="mailto:zhulinchao7@gmail.com" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a>
  
  
  
  <a href="http://linkedin.com/in/linchao-zhu-73b38366" class="author-social" target="_blank"><i class="fa fa-fw fa-linkedin-square"></i> LinkedIn</a>
  
  
  
  <a href="http://github.com/ffmpbgrnn" class="author-social" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a>
  
  
  
  
  
  
  
  
  
  
  
</div>

  </div>
  <article class="page">
    <!--<h1></h1>-->
    <div class="article-wrap">
      <p>Dr. Linchao Zhu (朱霖潮) is currently an Assistant Professor with the College of Computer Science at Zhejiang University. Before that, he was a Lecturer at the <a href="http://reler.net/">ReLER lab</a>, University of Technology Sydney. His research focus includes foundation models, multi-modal learning, and AI for scientific discovery. He received his Ph.D. degree in University of Technology Sydney, advised by Prof. Yi Yang. He graduated from Zhejiang University with a bachelor’s degree.</p>

<h3 id="news">News</h3>
<ul>
  <li>Jan 2024: We are organizing the Fifth Workshop on Neural Architecture Search at CVPR 2024.</li>
  <li>Jan 2024: Serving as an Area Chair for ECCV 2024.</li>
  <li>Jan 2024: Serving as an Area Chair for ICIP 2024.</li>
</ul>

<h3 id="preprints">Preprints</h3>
<ul>
  <li>
    <p>AntEval: Quantitatively Evaluating Informativeness and Expressiveness of Agent Social Interactions <br />
Yuanzhi Liang, <strong>Linchao Zhu</strong>, Yi Yang <br />
arXiv [<a href="https://arxiv.org/pdf/2401.06509.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax <br />
Yu Lu, <strong>Linchao Zhu</strong>, Hehe Fan, Yi Yang <br />
arXiv [<a href="https://arxiv.org/pdf/2311.15813">PDF</a>]</p>
  </li>
</ul>

<h3 id="publications">Publications</h3>
<ul>
  <li>
    <p>Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models <br />
Shuai Zhao, Xiaohan Wang, <strong>Linchao Zhu</strong>, Yi Yang <br />
ICLR 2024 [<a href="https://openreview.net/forum?id=kIP0duasBb">PDF</a>] [<a href="https://mzhaoshuai.github.io/RLCF/">Project page</a>]</p>
  </li>
  <li>
    <p>Temporal perceiving video-language pre-training <br />
Fan Ma, Xiaojie Jin, Heng Wang, Jingjia Huang, <strong>Linchao Zhu</strong>, Jiashi Feng, Yi Yang <br />
AAAI 2024 [<a href="https://arxiv.org/pdf/2301.07463.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval <br />
Xiangpeng Yang, <strong>Linchao Zhu</strong>, Xiaohan Wang, Yi Yang <br />
AAAI 2024 [<a href="https://arxiv.org/abs/2401.10588v1">PDF</a>]</p>
  </li>
  <li>
    <p>Text Augmented Spatial-aware Zero-shot Referring Image Segmentation <br />
Yucheng Suo, <strong>Linchao Zhu</strong>, Yi Yang <br />
EMNLP Findings 2023 [<a href="https://arxiv.org/pdf/2310.18049.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>MAAL: Multimodality-Aware Autoencoder-Based Affordance Learning for 3D Articulated Objects <br />
Yuanzhi Liang, Xiaohan Wang, <strong>Linchao Zhu</strong>, Yi Yang <br />
ICCV 2023 [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_MAAL_Multimodality-Aware_Autoencoder-Based_Affordance_Learning_for_3D_Articulated_Objects_ICCV_2023_paper.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings <br />
Wenjie Zhuo, Yifan Sun, Xiaohan Wang, <strong>Linchao Zhu</strong>, Yi Yang <br />
ACL 2023 (<strong>Oral</strong>) [<a href="https://aclanthology.org/2023.acl-long.677.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>Gloss-Free End-to-End Sign Language Translation <br />
Kezhou Lin, Xiaohan Wang, <strong>Linchao Zhu</strong>, Ke Sun, Bang Zhang, Yi Yang <br />
ACL 2023 (<strong>Oral</strong>) [<a href="https://arxiv.org/pdf/2305.12876">PDF</a>]</p>
  </li>
  <li>
    <p>Variational cross-graph reasoning and adaptive structured semantics learning for compositional temporal grounding <br />
Juncheng Li, Siliang Tang, <strong>Linchao Zhu</strong>, Wenqiao Zhang, Yi Yang, Tat-Seng Chua, Fei Wu, Yueting Zhuang <br />
TPAMI [<a href="https://ieeexplore.ieee.org/abstract/document/10121664">PDF</a>]</p>
  </li>
  <li>
    <p>PointListNet: Deep Learning on 3D Point Lists <br />
Hehe Fan, <strong>Linchao Zhu</strong>, Yi Yang, Mohan Kankanhalli <br />
CVPR 2023 [<a href="https://ffmpbgrnn.github.io/publications/pdf/pointlist.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>Efficient Multimodal Fusion via Interactive Prompting <br />
Yaowei Li, Ruijie Quan, <strong>Linchao Zhu</strong>, Yi Yang <br />
CVPR 2023 [<a href="https://ffmpbgrnn.github.io/publications/pdf/prompt.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>MIST: Multi-modal Iterative Spatial-Temporal Transformer for Long-form Video Question Answering <br />
Difei Gao, Luowei Zhou, Lei Ji, <strong>Linchao Zhu</strong>, Yi Yang, Mike Zheng Shou <br />
CVPR 2023 [<a href="https://arxiv.org/pdf/2212.09522.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training <br />
Wei Li, <strong>Linchao Zhu</strong>, Longyin Wen, Yi Yang <br />
ICLR 2023 [<a href="https://arxiv.org/abs/2303.03032">PDF</a>]</p>
  </li>
  <li>
    <p>Fine-Grained Semantically Aligned Vision-Language Pre-Training <br />
Juncheng Li, Xin He, Longhui Wei, Long Qian, <strong>Linchao Zhu</strong>, Lingxi Xie, Yueting Zhuang, Qi Tian, Siliang Tang <br />
NeurIPS 2022 [<a href="https://arxiv.org/abs/2208.02515">PDF</a>]</p>
  </li>
  <li>
    <p>CenterCLIP: Token Clustering for Efficient Text-Video Retrieval <br />
Shuai Zhao, <strong>Linchao Zhu</strong>, Xiaohan Wang, Yi Yang <br />
SIGIR 2022 [<a href="https://arxiv.org/abs/2205.00823">PDF</a>] [<a href="https://github.com/mzhaoshuai/CenterCLIP">Code</a>]</p>
  </li>
  <li>
    <p>Dilated Context Integrated Network with Cross-Modal Consensus for Temporal Emotion Localization in Videos
Juncheng Li, Junlin Xie, <strong>Linchao Zhu</strong>, Long Qian, Siliang Tang, Wenqiao Zhang, Haochen Shi, Shengyu Zhang, Longhui Wei, Qi Tian, Yueting Zhuang <br />
ACM MM 2022. [<a href="https://arxiv.org/abs/2208.01954">PDF</a>]</p>
  </li>
  <li>
    <p>A Simple Episodic Linear Probe Improves Visual Recognition in the Wild <br />
Yuanzhi Liang, <strong>Linchao Zhu</strong>, Xiaohan Wang, Yi Yang <br />
CVPR 2022 [<a href="https://ffmpbgrnn.github.io/publications/pdf/elp.pdf">PDF</a>] [<a href="https://github.com/akira-l/ELP">Code</a>]</p>
  </li>
  <li>
    <p>Unified Transformer Tracker for Object Tracking <br />
Fan Ma, Mike Zheng Shou, <strong>Linchao Zhu</strong>, Haoqi Fan, Yilei Xu, Yi Yang, Zhicheng Yan <br />
CVPR 2022 [<a href="https://arxiv.org/abs/2203.15175">PDF</a>] [<a href="https://github.com/flowerfan/trackron">Code</a>]</p>
  </li>
  <li>
    <p>SEEG: Semantic Energized Co-speech Gesture Generation <br />
Yuanzhi Liang, Qianyu Feng, <strong>Linchao Zhu</strong>, Li Hu, Pan Pan, Yi Yang <br />
CVPR 2022 [<a href="https://ffmpbgrnn.github.io/publications/pdf/seeg.pdf">PDF</a>] [<a href="https://github.com/akira-l/SEEG">Code</a>]</p>
  </li>
  <li>
    <p>Complex Video Action Reasoning via Learnable Markov Logic Network <br />
Yang Jin, <strong>Linchao Zhu</strong>, Yadong Mu <br />
CVPR 2022 [<a href="https://ffmpbgrnn.github.io/publications/pdf/mln.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning <br />
Juncheng Li, Junlin Xie, Long Qian, <strong>Linchao Zhu</strong>, Siliang Tang, Fei Wu, Yi Yang, Yueting Zhuang, Xin Eric Wang <br />
CVPR 2022 [<a href="https://arxiv.org/abs/2203.13049">PDF</a>] [<a href="https://github.com/YYJMJC/Compositional-Temporal-Grounding">Code</a>]</p>
  </li>
  <li>
    <p>Weakly Supervised Moment Localization with Decoupled Consistent Concept Prediction <br />
Fan Ma, <strong>Linchao Zhu</strong>, Yi Yang <br />
IJCV 2022 [<a href="https://link.springer.com/article/10.1007/s11263-022-01600-0">PDF</a>]</p>
  </li>
  <li>
    <p>Interactive Prototype Learning for Egocentric Action Recognition <br />
Xiaohan Wang, <strong>Linchao Zhu</strong>, Heng Wang, Yi Yang <br />
ICCV 2021 [<a href="https://ffmpbgrnn.github.io/publications/pdf/ipl.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>A Multi-Mode Modulator for Multi-Domain Few-Shot Classification <br />
Yanbin Liu, Juho Lee, <strong>Linchao Zhu</strong>, Ling Chen, Humphrey Shi, Yi Yang <br />
ICCV 2021 [<a href="https://csyanbin.github.io/papers/ICCV2021_tri-M.pdf">PDF</a>][<a href="https://csyanbin.github.io/papers/ICCV2021_tri-M-supp.pdf">Supp</a>] [<a href="https://github.com/csyanbin/tri-M-ICCV">Code</a>]</p>
  </li>
  <li>
    <p>Universal-Prototype Enhancing for Few-Shot Object Detection <br />
Aming Wu, Yahong Han, <strong>Linchao Zhu</strong>, Yi Yang <br />
ICCV 2021 [<a href="https://arxiv.org/abs/2103.01077">PDF</a>] [<a href="https://github.com/AmingWu/UP-FSOD">Code</a>]</p>
  </li>
  <li>
    <p>Adaptive Hierarchical Graph Reasoning with Semantic Coherence for Video-and-Language Inference <br />
Juncheng Li, Siliang Tang, <strong>Linchao Zhu</strong>, Haochen Shi, Xuanwen Huang, Fei Wu, Yi Yang, Yueting Zhuang <br />
ICCV 2021 [<a href="https://arxiv.org/abs/2107.12270">PDF</a>]</p>
  </li>
  <li>
    <p>Vector-Decomposed Disentanglement for Domain-Invariant Object Detection <br />
Aming Wu, Rui Liu, Yahong Han, <strong>Linchao Zhu</strong>, Yi Yang <br />
ICCV 2021 [<a href="https://arxiv.org/abs/2108.06685">PDF</a>] [<a href="https://github.com/AmingWu/VDD-DAOD">Code</a>]</p>
  </li>
  <li>
    <p>Faster Meta Update Strategy for Noise-Robust Deep Learning <br />
Youjiang Xu, <strong>Linchao Zhu</strong>, Lu Jiang, Yi Yang <br />
CVPR 2021 (<strong>Oral</strong>) [<a href="http://ffmpbgrnn.github.io/publications/pdf/famus.pdf">PDF</a>] [<a href="http://ffmpbgrnn.github.io/publications/pdf/famus-supp.pdf">Supp</a>] [<a href="https://github.com/youjiangxu/FaMUS">Code</a>]</p>
  </li>
  <li>
    <p>OpenMix: Reviving Known Knowledge for Discovering Novel Visual Categories in An Open World <br />
Zhun Zhong, <strong>Linchao Zhu</strong>, Zhiming Luo, Shaozi Li, Yi Yang, Nicu Sebe <br />
CVPR 2021 [<a href="http://ffmpbgrnn.github.io/publications/pdf/openmix.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>T2VLAD: Global-Local Sequence Alignment for Text-Video Retrieval <br />
Xiaohan Wang, <strong>Linchao Zhu</strong>, Yi Yang <br />
CVPR 2021 [<a href="http://ffmpbgrnn.github.io/publications/pdf/t2vlad.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>Instance-Invariant Domain Adaptive Object Detection via Progressive Disentanglement <br />
Aming Wu, Yahong Han, <strong>Linchao Zhu</strong>, Yi Yang <br />
TPAMI, DOI: 10.1109/TPAMI.2021.3060446 [<a href="https://ieeexplore.ieee.org/document/9362301">PDF</a>] [<a href="https://github.com/AmingWu/IIOD">Code</a>]</p>
  </li>
  <li>
    <p>Symbiotic Attention for Egocentric Action Recognition with Object-centric Alignment <br />
Xiaohan Wang, <strong>Linchao Zhu</strong>, Yu Wu, Yi Yang <br />
TPAMI, DOI: 10.1109/TPAMI.2020.3015894 [<a href="https://ieeexplore.ieee.org/document/9165005">PDF</a>][<a href="https://ffmpbgrnn.github.io/publications/bib/sa.txt">Bibtex</a>]</p>
  </li>
  <li>
    <p>Label Independent Memory for Semi-Supervised Few-shot Video Classification <br />
<strong>Linchao Zhu</strong>, Yi Yang <br />
TPAMI, DOI: 10.1109/TPAMI.2020.3007511, 2020 [<a href="https://ieeexplore.ieee.org/document/9134951/">PDF</a>][<a href="https://ffmpbgrnn.github.io/publications/bib/lim.txt">Bibtex</a>]</p>
  </li>
  <li>
    <p>SF-Net: Single-Frame Supervision for Temporal Action Localization <br />
Fan Ma, <strong>Linchao Zhu</strong>, Yi Yang, Shengxin Zha, Gourab Kundu, Matt Feiszli, Zheng Shou <br />
ECCV 2020 (<strong>Spotlight</strong>) [<a href="https://arxiv.org/pdf/2003.06845.pdf">PDF</a>][<a href="https://github.com/Flowerfan/SF-Net">Code</a>]</p>
  </li>
  <li>
    <p>Motion-Excited Sampler: Video Adversarial Attack with Sparked Prior <br />
Hu Zhang, <strong>Linchao Zhu</strong>, Yi Zhu, Yi Yang <br />
ECCV 2020 [<a href="https://arxiv.org/pdf/2003.07637.pdf">PDF</a>][<a href="https://github.com/xiaofanustc/ME-Sampler">Code</a>]</p>
  </li>
  <li>
    <p>Learning to Transfer Learn: Reinforcement Learning-Based Selection for Adaptive Transfer Learning <br />
<strong>Linchao Zhu</strong>, Sercan O. Arık, Yi Yang, Tomas Pfister <br />
ECCV 2020 [<a href="https://arxiv.org/pdf/1908.11406.pdf">PDF</a>] [<a href="https://github.com/google-research/google-research/tree/master/l2tl">Code</a>]</p>
  </li>
  <li>
    <p>ActBERT: Learning Global-Local Video-Text Representations <br />
<strong>Linchao Zhu</strong>, Yi Yang <br />
CVPR 2020 (<strong>Oral</strong>) [<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_ActBERT_Learning_Global-Local_Video-Text_Representations_CVPR_2020_paper.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>Inflated Episodic Memory with Region Self-Attention for Long-Tailed Visual Recognition <br />
<strong>Linchao Zhu</strong>, Yi Yang <br />
CVPR 2020 [<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Inflated_Episodic_Memory_With_Region_Self-Attention_for_Long-Tailed_Visual_Recognition_CVPR_2020_paper.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>Gated Channel Transformation for Visual Recognition <br />
Zongxin Yang, <strong>Linchao Zhu</strong>, Yu Wu, Yi Yang <br />
CVPR 2020 [<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Gated_Channel_Transformation_for_Visual_Recognition_CVPR_2020_paper.pdf">Arxiv</a>][<a href="https://github.com/z-x-yang/GCT">Code</a>]</p>
  </li>
  <li>
    <p>Semantic Correspondence as an Optimal Transport Problem <br />
Yanbin Liu, <strong>Linchao Zhu</strong>, Makoto Yamada, Yi Yang <br />
CVPR 2020 [<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Semantic_Correspondence_as_an_Optimal_Transport_Problem_CVPR_2020_paper.pdf">PDF</a>][<a href="https://github.com/csyanbin/SCOT">Code</a>]</p>
  </li>
  <li>
    <p>Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration  <br />
Yang He, Yuhang Ding, Ping Liu, <strong>Linchao Zhu</strong>, Hanwang Zhang, Yi Yang <br />
CVPR 2020 [<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/He_Learning_Filter_Pruning_Criteria_for_Deep_Convolutional_Neural_Networks_Acceleration_CVPR_2020_paper.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>Symbiotic Attention with Privileged Information for Egocentric Action Recognition <br />
Xiaohan Wang, Yu Wu, <strong>Linchao Zhu</strong>, Yi Yang <br />
AAAI 2020 (<strong>Oral</strong>) [<a href="http://ffmpbgrnn.github.io/publications/pdf/epic-aaai.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>FASTER Recurrent Networks for Video Classification <br />
<strong>Linchao Zhu</strong>, Laura Sevilla-Lara, Du Tran, Matt Feiszli, Yi Yang, Heng Wang <br />
AAAI 2020 [<a href="https://www.aaai.org/Papers/AAAI/2020GB/AAAI-ZhuL.589.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>Connective Cognition Network for Directional Visual Commonsense Reasoning <br />
Aming Wu, <strong>Linchao Zhu</strong>, Yahong Han, Yi Yang <br />
NeurIPS 2019 [<a href="https://papers.nips.cc/paper/8804-connective-cognition-network-for-directional-visual-commonsense-reasoning.pdf">PDF</a> <a href="https://github.com/AmingWu/CCN">Code</a>]</p>
  </li>
  <li>
    <p>Dual Attention Matching for Audio-Visual Event Localization <br />
Yu Wu, <strong>Linchao Zhu</strong>, Yan Yan, Yi Yang <br />
ICCV 2019 (<strong>Oral</strong>) [<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Dual_Attention_Matching_for_Audio-Visual_Event_Localization_ICCV_2019_paper.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>Entangled Transformer for Image Captioning <br />
Guang Li, <strong>Linchao Zhu</strong>, Ping Liu, Yi Yang <br />
ICCV 2019 [<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>Auto-ReID: Searching for a Part-Aware ConvNet for Person Re-Identification <br />
Ruijie Quan, Xuanyi Dong, Yu Wu, <strong>Linchao Zhu</strong>, Yi Yang <br />
ICCV 2019 [<a href="https://arxiv.org/abs/1903.09776">PDF</a>]</p>
  </li>
  <li>
    <p>Sim-Real Joint Reinforcement Transfer for 3D Indoor Navigation  <br />
Fengda Zhu, <strong>Linchao Zhu</strong>, Yi Yang <br />
CVPR 2019 [<a href="https://arxiv.org/abs/1904.03895">PDF</a>]</p>
  </li>
  <li>
    <p>Cubic LSTMs for Video Prediction  <br />
Hehe Fan, <strong>Linchao Zhu</strong>, Yi Yang <br />
AAAI 2019 [<a href="https://hehefan.github.io/pdfs/CubicLSTM.pdf">PDF</a>]</p>
  </li>
  <li>
    <p>Compound Memory Networks for Few-shot Video Classification <br />
<strong>Linchao Zhu</strong>, Yi Yang <br />
ECCV 2018 [<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Linchao_Zhu_Compound_Memory_Networks_ECCV_2018_paper.pdf">PDF</a>], [<a href="http://ffmpbgrnn.github.io/publications/CMN/train.list">train.list</a>, <a href="http://ffmpbgrnn.github.io/publications/CMN/val.list">val.list</a>, <a href="http://ffmpbgrnn.github.io/publications/CMN/test.list">test.list</a>]</p>
  </li>
  <li>
    <p>Decoupled Novel Object Captioner <br />
Yu Wu, <strong>Linchao Zhu</strong>, Lu Jiang, Yi Yang <br />
ACM MM 2018 [<a href="https://arxiv.org/abs/1804.03803">PDF</a> <a href="https://github.com/Yu-Wu/Decoupled-Novel-Object-Captioner">Code</a>]</p>
  </li>
  <li>
    <p>Fast Parameter Adaptation for Few-shot Image Captioning and Visual Question Answering <br />
Xuanyi Dong, <strong>Linchao Zhu</strong>, De Zhang, Yi Yang, Fei Wu <br />
ACM MM 2018 [<a href="https://dl.acm.org/citation.cfm?id=3240527">PDF</a> <a href="https://github.com/D-X-Y/FPAIT">Code</a>]</p>
  </li>
  <li>
    <p>Watching a Small Portion could be as Good as Watching All: Towards Efficient Video Classification <br />
Hehe Fan, Zhongwen Xu, <strong>Linchao Zhu</strong>, Chenggang Yan, Jianjun Ge, Yi Yang <br />
IJCAI 2018 [<a href="https://www.ijcai.org/proceedings/2018/0098.pdf">PDF</a> <a href="https://github.com/hehefan/video-classification">Code</a>]</p>
  </li>
  <li>
    <p>Bidirectional Multirate Reconstruction for Temporal Modeling in Videos <br />
<strong>Linchao Zhu</strong>, Zhongwen Xu, Yi Yang <br />
CVPR 2017 (<strong>Spotlight</strong>) [<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Bidirectional_Multirate_Reconstruction_CVPR_2017_paper.pdf">PDF</a> <a href="https://github.com/ffmpbgrnn/tflibs">Code</a>]</p>
  </li>
  <li>
    <p>Few-Shot Object Recognition from Machine-Labeled Web Images  <br />
Zhongwen Xu*, <strong>Linchao Zhu</strong>*, Yi Yang <br />
CVPR 2017 (<strong>Spotlight</strong>), * indicates equal contributions [<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Xu_Few-Shot_Object_Recognition_CVPR_2017_paper.pdf">PDF</a> <a href="https://github.com/ffmpbgrnn/tflibs">Code</a>]</p>
  </li>
  <li>
    <p>Uncovering Temporal Context for Video Question Answering<br />
<strong>Linchao Zhu</strong>, Zhongwen Xu, Yi Yang, Alex G. Hauptmann <br />
IJCV, DOI: 10.1007/s11263-017-1033-7, 2017 [<a href="https://link.springer.com/article/10.1007/s11263-017-1033-7">PDF</a>] [<a href="https://github.com/ffmpbgrnn/VideoQA">Project</a>] [<a href="https://ffmpbgrnn.github.io/publications/bib/vqa.txt">Bibtex</a>]</p>
  </li>
  <li>
    <p>Recognizing an action using its name: A knowledge-based approach <br />
Chuang Gan, Yi Yang, <strong>Linchao Zhu</strong>, Deli Zhao, Yueting Zhuang <br />
IJCV, DOI: 10.1007/s11263-016-0893-6, 2016 [<a href="https://link.springer.com/article/10.1007/s11263-016-0893-6">PDF</a>]</p>
  </li>
</ul>

<h3 id="competitions">Competitions</h3>
<ul>
  <li>
    <p>The first place on behaviour representation learning from video data at MABe 2022, CVPR [<a href="https://www.aicrowd.com/challenges/multi-agent-behavior-challenge-2022/problems/mabe-2022-mouse-triplets-video-data">Mouse Triplets Track</a>] [<a href="https://www.aicrowd.com/challenges/multi-agent-behavior-challenge-2022/problems/mabe-2022-ant-beetles-video-data">Ant-beetle Groups Track</a>].</p>
  </li>
  <li>
    <p>The second place in the <a href="https://sites.google.com/view/auvi-cvpr2021/challenge">Evoked Emotion from Videos Challenge</a>, CVPR 2021.  See our <a href="https://arxiv.org/abs/2106.01764">technical report</a> and <a href="https://github.com/HenryLittle/EEV-Challenge-2021">code</a>.</p>
  </li>
  <li>
    <p>The first place in <a href="https://epic-kitchens.github.io/2020">EPIC-Kitchen Action Recognition 2020</a>. See our <a href="http://ffmpbgrnn.github.io/publications/pdf/epic_report2020.pdf">report</a> for more details.</p>
  </li>
  <li>
    <p>The first place in <a href="https://epic-kitchens.github.io/2018">EPIC-Kitchen Action Recognition 2019</a>. See our <a href="https://arxiv.org/abs/1906.09383">report</a>.</p>
  </li>
  <li>
    <p>Our <a href="http://ffmpbgrnn.github.io/publications/pdf/act2017.pdf">report</a> on ActivityNet Trimmed Action Recognition 2017.</p>
  </li>
  <li>
    <p>Our <a href="http://ffmpbgrnn.github.io/publications/pdf/yt8m.pdf">report</a> on Google YouTube8M Classification 2017.</p>
  </li>
  <li>
    <p>The first place in the video localization competition, TRECVID 2016.
See our <a href="http://ffmpbgrnn.github.io/publications/pdf/loc.pdf">report</a>.</p>
  </li>
  <li>
    <p>The first place in the Action Recognition competition, THUMOS 2015.
See our <a href="http://www.cs.cmu.edu/~zhongwen/pdf/THUMOS15.pdf">notebook paper</a>.</p>
  </li>
</ul>

      
    </div><!-- /.article-wrap -->
    
  </article>
</div><!-- /#index -->

<div class="footer-wrap">
  <footer>
    

<span>&copy; 2024 Linchao Zhu. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://ffmpbgrnn.github.io//assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://ffmpbgrnn.github.io//assets/js/scripts.min.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-47828206-2', 'auto');
  ga('send', 'pageview');

</script>




</body>
</html>
